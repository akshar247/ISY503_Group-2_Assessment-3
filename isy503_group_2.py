# -*- coding: utf-8 -*-
"""ISY503_Group_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IxgfIY2zp3mVbSI7EVkBepE8lzf-TwZx
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/MyDrive/ISY503/domain_sentiment_data/sorted_data_acl/

# Commented out IPython magic to ensure Python compatibility.
# %ls


import pandas as pd
from bs4 import BeautifulSoup
import os


def extract_all_reviews(folder_path, filename):
    data = {'Review': []}

    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if os.path.isfile(file_path) and file_path.endswith(".review"):
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
                beautiSoup = BeautifulSoup(content, 'html.parser')
                reviewTags = beautiSoup.find_all('review_text')
                for review_tag in reviewTags:
                    reviewText = review_tag.get_text().strip()
                    data['Review'].append(reviewText)

    df = pd.DataFrame(data)
    return df


negative_dvd_review = extract_all_reviews('./dvd', 'negative.review')
positive_dvd_review = extract_all_reviews('./dvd', 'positive.review')
negative_books_review = extract_all_reviews('./books', 'negative.review')
positive_books_review = extract_all_reviews('./books', 'positive.review')
negative_electronics_review = extract_all_reviews('./electronics', 'negative.review')
positive_electronics_review = extract_all_reviews('./electronics', 'positive.review')
negative_kitchen_review = extract_all_reviews('./kitchen_&_housewares', 'negative.review')
positive_kitchen_review = extract_all_reviews('./kitchen_&_housewares', 'positive.review')

negative_dvd_review["label"] = 0
positive_dvd_review["label"] = 1
negative_books_review["label"] = 0
positive_books_review["label"] = 1
negative_electronics_review["label"] = 0
positive_electronics_review["label"] = 1
negative_kitchen_review["label"] = 0
positive_kitchen_review["label"] = 1

negative_dvd_review["type"] = "dvd"
positive_dvd_review["type"] = "dvd"
negative_books_review["type"] = "books"
positive_books_review["type"] = "books"
negative_electronics_review["type"] = "electronics"
positive_electronics_review["type"] = "electronics"
negative_kitchen_review["type"] = "kitchen"
positive_kitchen_review["type"] = "kitchen"

frames = [negative_dvd_review, positive_dvd_review, negative_books_review, positive_books_review,
          negative_electronics_review, positive_electronics_review, negative_kitchen_review, positive_kitchen_review]
result = pd.concat(frames)

df = result.sample(frac=1)
df

import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
from joblib import Parallel, delayed

df['Review'] = df['Review'].apply(lambda x: x.lower())

# !pip install nltk

import nltk

nltk.download("stopwords")

nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


def remove_stopWords(text):
    stopWords = set(stopwords.words('english'))
    word = word_tokenize(text)
    filtered_word = [word for word in word if word.lower() not in stopWords]
    return ' '.join(filtered_word)

    df['Review'] = df['Review'].apply(lambda x: remove_stopWords(x))


label_encoder = LabelEncoder()
df['Label'] = label_encoder.fit_transform(df['label'])

df['Review_Length'] = df['Review'].apply(lambda x: len(x.split()))
df = df[df['Review_Length'] > 2]

token = Tokenizer()
token.fit_on_texts(df['Review'])
encoded_reviews = token.texts_to_sequences(df['Review'])

joblib.dump(token, 'token.pkl')

maxLength = 20
paddedReviews = pad_sequences(encoded_reviews, maxlen=maxLength, padding='post', truncating='post')

train_data, test_data, train_labels, test_labels = train_test_split(paddedReviews, df['Label'], test_size=0.2,
                                                                    random_state=42)
train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2,
                                                                  random_state=42)

embedding_dim = 50
model = Sequential()
model.add(Embedding(input_dim=len(token.word_index) + 1, output_dim=embedding_dim, input_length=maxLength))
model.add(LSTM(100))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

earlyStopping = EarlyStopping(patience=3, restore_best_weights=True)
model.fit(train_data, train_labels, epochs=10, validation_data=(val_data, val_labels), callbacks=[earlyStopping])

test_loss, test_accuracy = model.evaluate(test_data, test_labels)
print("Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

joblib.dump(model, 'model.pkl')

joblib_model = joblib.load('model.pkl')

encoded_sentence = token.texts_to_sequences([input_sentence])
padded_sentence = pad_sequences(encoded_sentence, maxlen=maxLength, padding='post', truncating='post')
predicted_sentiment = joblib_model.predict(padded_sentence)

predicted_sentiment
